# -*- coding: utf-8 -*-
"""NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U7zYGFCEAxCbNxkJ8s50wdBHyY5opxYk
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.metrics import accuracy_score, confusion_matrix , classification_report ,precision_score,recall_score, f1_score
from sklearn.model_selection import train_test_split
import math
import seaborn as sns
import random
from IPython import display

np.random.seed(0)

#Data
num_samples = 1000
num_features = 2
num_classes = 5
random_seed = 35

#synthetic data
x, y = make_blobs(n_samples=num_samples, n_features=num_features, centers=num_classes, random_state=random_seed)
x = np.abs(x)
y = np.abs(y)

# Split the dataset
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=random_seed)

#class names
propertyTypes = ['Budget', 'Affordable', 'Mid-range', 'Premium', 'Luxury']

# Plot the dataset
plt.figure(figsize=(10, 6))
scatter_plot = plt.scatter(features[:, 0], features[:, 1], c=labels, cmap=plt.cm.get_cmap('plasma', num_classes), marker='o', s=20)

plt.colorbar(scatter_plot, ticks=range(num_classes)).set_ticklabels(propertyTypes[::-1])
plt.title('Synthetic Dataset with 5 Classes')
plt.xlabel('Property Size')
plt.ylabel('Property Price Level')

plt.show()

features_train

features_test

labels_train

labels_test

print("Shapes of Each Segment in the Dataset:")
print("Training Features Shape:", features_train.shape)
print("Training Labels Shape:", labels_train.shape)
print("Testing Features Shape:", features_test.shape)
print("Testing Labels Shape:", labels_test.shape)

class NeuralNetwork(object):
    def __init__(self):
        iLn = 2
        hLN1 = 500
        hLN2 = 500
        hLN3 = 500
        oLn = num_classes  # Assuming num_classes is defined earlier
        self.eta = 0.1

        # Weights
        self.wHL1 = np.random.randn(iLn, hLN1)
        self.wHL2 = np.random.randn(hLN1, hLN2)
        self.wHL3 = np.random.randn(hLN2, hLN3)
        self.WO = np.random.randn(hLN3, oLn)

        # Biases
        self.b1 = np.zeros((1, hLN1))
        self.b2 = np.zeros((1, hLN2))
        self.b3 = np.zeros((1, hLN3))
        self.WB = np.zeros((1, oLn))

    def sigmoid(self, features):
        return 1 / (1 + np.exp(-features))

    def sigmoid_dev(self, features):
        return  features * (1-features)

    def softmax(self, features):
        exp_x = np.exp(features - np.max(features, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)

    def ForwardProp(self, features):
        self.HO1 = self.sigmoid(np.dot(features, self.wHL1) + self.b1)
        self.HO2 = self.sigmoid(np.dot(self.HO1, self.wHL2) + self.b2)
        self.HO3 = self.sigmoid(np.dot(self.HO2, self.wHL3) + self.b3)
        self.output = self.softmax(np.dot(self.HO3, self.WO) + self.WB)
        return self.output

    def BackProp(self, features, labels):
        m = features.shape[0]

        # Output layer error
        diff_out = self.output - labels
        d_WO = (1/m) * np.dot(self.HO3.T, diff_out)
        d_WB = (1/m) * np.sum(diff_out, axis=0, keepdims=True)

        # Backprop through layers
        d_hidden3 = np.dot(diff_out, self.WO.T) * self.sigmoid_dev(self.HO3)
        d_W_HI3 = (1/m) * np.dot(self.HO2.T, d_hidden3)
        d_b3 = (1/m) * np.sum(d_hidden3, axis=0, keepdims=True)

        d_hidden2 = np.dot(d_hidden3, self.wHL3.T) * self.sigmoid_dev(self.HO2)
        d_W_HI2 = (1/m) * np.dot(self.HO1.T, d_hidden2)
        d_b2 = (1/m) * np.sum(d_hidden2, axis=0, keepdims=True)

        d_hidden1 = np.dot(d_hidden2, self.wHL2.T) * self.sigmoid_dev(self.HO1)
        d_W_HI1 = (1/m) * np.dot(features.T, d_hidden1)
        d_b1 = (1/m) * np.sum(d_hidden1, axis=0, keepdims=True)

        # Update weights and biases
        self.WO -= self.eta * d_WO
        self.WB -= self.eta * d_WB
        self.wHL3 -= self.eta * d_W_HI3
        self.b3 -= self.eta * d_b3
        self.wHL2 -= self.eta * d_W_HI2
        self.b2 -= self.eta * d_b2
        self.wHL1 -= self.eta * d_W_HI1
        self.b1 -= self.eta * d_b1

    def predict(self, features):
        probabilities = self.ForwardProp(features)
        return np.argmax(probabilities, axis=1)

    @staticmethod
    def ReLU(features, derivative=False):
        if derivative:
            return np.where(features <= 0, 0, 1)
        else:
            return np.maximum(0, features)

NN = NeuralNetwork()

def one_hot_encode(labels, num_classes):
    return np.eye(num_classes)[labels]

labels_train_one_hot = one_hot_encode(labels_train, num_classes)

err = []
epochs = 10000

for i in range(epochs):
    output = NN.ForwardProp(features_train)
    NN.BackProp(features_train, labels_train_one_hot)

    # Calculating loss using cross-entropy
    loss = np.mean(-np.sum(labels_train_one_hot * np.log(output + 1e-9), axis=1))  # Added a small epsilon to avoid log(0)
    err.append(loss)

    if i % 100 == 0:
        print(f'Epoch {i}, Loss: {loss:.4f}')

plt.plot(err)
plt.xlabel('Epochs ->')
plt.ylabel('Loss ->')
plt.title('Training Loss Over Time')
plt.show()

# testing
yPred = NN.ForwardProp(features_test)
plt.plot(yPred)

# test 1
size1 = 15
price1 = 15
test1 = NN.ForwardProp([[size1, price1]])

predicted_class = np.argmax(test1)

print("Classes with Probabilities:\n")
for i, name in enumerate(propertyTypes):
    print(f"Class {i+1} ({name}): {test1[0][i]:.4f}")

print("\nPredicted Class:")
print(f"{propertyTypes[predicted_class]} ({test1[0][predicted_class]:.4f})")

# test 2
size2 = 30
price2 = 30
test2 = NN.ForwardProp([[size2, price2]])

predicted_class = np.argmax(test2)

print("Classes with Probabilities:\n")
for i, name in enumerate(propertyTypes):
    print(f"Class {i+1} ({name}): {test2[0][i]:.4f}")

print("\nPredicted Class:")
print(f"{propertyTypes[predicted_class]} ({test2[0][predicted_class]:.4f})")

yPredCls = np.argmax(yPred, axis=1)

#Accuracy
accuracy = accuracy_score(labels_test, yPredCls)
print("Accuracy of this model :", accuracy*100,'%')