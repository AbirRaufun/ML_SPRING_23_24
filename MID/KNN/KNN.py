# -*- coding: utf-8 -*-
"""Class Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zA_NJfCM_Sqb0O_OYajflF4grWGrRn_S

Importing library
"""

import tensorflow as tf
from tensorflow.keras import datasets,layers,models
import matplotlib.pyplot as plt
import numpy as np

"""**Downloading data and also train **"""

(X_train,y_train),(X_test,y_test) = datasets.cifar10.load_data()
X_train.shape

"""**Showing the size of training and test data **"""

print('Training data shape: ', X_train.shape)
print('Training labels shape: ', y_train.shape)
print('Test data shape: ', X_test.shape)
print('Test labels shape: ', y_test.shape)

"""**Configuring Default Parameters to show the images"""

plt.rcParams['figure.figsize'] = (12.0, 8.0)
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
num_classes = len(classes)
samples_per_class = 7
for y, cls in enumerate(classes):
    idxs = np.flatnonzero(y_train == y)
    idxs = np.random.choice(idxs, samples_per_class, replace=False)
    for i, idx in enumerate(idxs):
        plt_idx = i * num_classes + y + 1
        plt.subplot(samples_per_class, num_classes, plt_idx)
        plt.imshow(X_train[idx].astype('uint8'))
        plt.axis('off')
        if i == 0:
            plt.title(cls)
plt.show()

"""**training and testing a model**"""

num_training = 10000
mask = list(range(num_training))
X_train = X_train[mask]
y_train = y_train[mask]

num_test = 1000
mask = list(range(num_test))
X_test = X_test[mask]
y_test = y_test[mask]

# Reshape the image data into rows
X_train = np.reshape(X_train, (X_train.shape[0], -1))
X_test = np.reshape(X_test, (X_test.shape[0], -1))
print(X_train.shape, X_test.shape)

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=10)

knn.fit(X_train, y_train)

dists, indices = knn.kneighbors(X_test)
print(dists.shape)

plt.imshow(dists, interpolation='none')
plt.show()

import seaborn as sns
sns.set()
with sns.axes_style('white'):
  plt.imshow(dists, aspect='auto', cmap='hot')
  plt.colorbar()

Z = knn.score(X_test, y_test)

print(Z*100)

classifier = KNeighborsClassifier(n_neighbors=5)

classifier.fit(X_train, y_train)

"""**At K=1 the score is the highest**"""

A = classifier.score(X_test, y_test)

print(A*100)

knn = KNeighborsClassifier(n_neighbors=10)
classifier = KNeighborsClassifier(n_neighbors=5)
classification = KNeighborsClassifier(n_neighbors=1)

classification.fit(X_train, y_train)

B = classification.score(X_test, y_test)
print(A*100)
print(B*100)
print(Z*100)

"""**From K= 1 to 20 to see the accuracy value  **"""

from sklearn.metrics import accuracy_score
for k in range (1,20):
    knn_classifier = KNeighborsClassifier(n_neighbors=k,metric='minkowski')
    knn_classifier.fit(X_train,y_train)
    y_knn = knn_classifier.predict(X_test)
    acc_score = accuracy_score(y_knn,y_test)
    print('Accuracy for k={0} => {1}'.format(k,acc_score * 100))

"""**Accuracy Score and Confusion matrix to see the satate of knn **"""

from sklearn.metrics import confusion_matrix,accuracy_score
accuracy_knn = accuracy_score(y_knn,y_test)
confusion_knn = confusion_matrix(y_knn,y_test)
print('Accuracy Score= ',accuracy_knn)
print('confusion_matrix= \n',confusion_knn)

"""Cross-validation
We have implemented the k-Nearest Neighbor classifier but we set the value k = 5 arbitrarily. We will now determine the best value of this hyperparameter with cross-validation
"""

from sklearn.metrics import accuracy_score
num_folds = 5
k_choices = [1, 3, 5, 7, 8, 10, 12, 15, 20, 50, 100]
X_train_folds = []
y_train_folds = []
y_train_ = y_train.reshape(-1, 1)
X_train_folds , y_train_folds = np.array_split(X_train, 10), np.array_split(y_train_,10)
k_to_accuracies = {}
for k_ in k_choices:
  k_to_accuracies.setdefault(k_, [])


for i in range(num_folds):
  classifier = KNeighborsClassifier()
  X_val_train = np.vstack(X_train_folds[0:i] + X_train_folds[i+1:])
  y_val_train = np.vstack(y_train_folds[0:i] + y_train_folds[i+1:])
  y_val_train = y_val_train[:,0]
  classifier.fit(X_val_train, y_val_train)
  for k_ in k_choices:
    y_val_pred = classifier.predict_proba(X_train_folds[i])
    num_correct = np.sum(y_val_pred == y_train_folds[i])
    accuracy = float(num_correct) / len(y_val_pred)
    k_to_accuracies[k_] = k_to_accuracies[k_] + [accuracy]

for k in sorted(k_to_accuracies):
  for accuracy in k_to_accuracies[k]:
    print ('k=%d,accuracy=%f'%(k,accuracy*100))

"""**Ploting the Cross validation to judge the accuracy**


"""

for k in k_choices:
  accuracies = k_to_accuracies[k]
  plt.scatter([k] * len(accuracies),accuracies)

accuracies_mean = np.array([np.mean(v) for k,v in sorted(k_to_accuracies.items())])
accuracies_std = np.array([np.std(v) for k,v in sorted(k_to_accuracies.items())])
plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)
plt.title('Cross-validation on k')
plt.xlabel('k')
plt.ylabel('Cross-validation accuracy')
plt.show()

#Manhattan

from sklearn.neighbors import KNeighborsClassifier

print("Using Manhattan Distance Metric")
classifier = KNeighborsClassifier(n_neighbors=5, metric='manhattan')
classifier.fit(X_train, y_train)

y_test_pred = classifier.predict(X_test)

# Calculate accuracy
num_correct = np.sum(y_test_pred == y_test.squeeze())
accuracy = float(num_correct) / len(y_test)
print(f'Got {num_correct} / {len(y_test)} correct with k=5 => accuracy: {accuracy}')

